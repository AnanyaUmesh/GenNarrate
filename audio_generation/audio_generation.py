# -*- coding: utf-8 -*-
"""AudioGen1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KT0_Z2_RFtkb21D3fqCLsSBXHf6KC_D2
"""

# !pip install -q git+https://github.com/huggingface/diffusers.git transformers accelerate scipy torch phonemizer
# !pip install -q git+https://github.com/facebookresearch/audiocraft.git
# !pip install ffmpeg pydub torchaudio

# !apt-get install espeak
# !pip install gtts

import os
import torch
import scipy
import re
from diffusers import AudioLDM2Pipeline
from gtts import gTTS
from pydub import AudioSegment
from pydub.effects import normalize, low_pass_filter
from textwrap import wrap
from google.colab import files



#Step 1: Define the Long Story
story_text = """A rabbit and a turtle lived in the same forest. They had always been rivals, constantly competing against each other in every activity they could think of. One day, they decided to have a race to see who was the faster one. The rabbit was confident that he would win easily, while the turtle was determined to prove that slow and steady can sometimes be better than fast and reckless.

The race began at the edge of the forest and both competitors sprinted as fast as they could towards the finish line. The rabbit zigzagged through the trees, using his quick reflexes and agility to gain an advantage, while the turtle plodded along steadily, never stopping or looking back.

As the race progressed, the rabbit grew tired and slowed down, but the turtle kept moving forward at a steady pace. In the end, the turtle crossed the finish line first, leaving the rabbit far behind.

The moral of the story is that slow and steady wins the race. The turtle may not have been as fast as the rabbit, but he never gave up and kept going until the end. He proved that hard work and determination are often more important than natural ability or talent.

"""  # a long story

# Step 2: Chunk the Story into Manageable Parts (250 chars per chunk)
#text_chunks = wrap(story_text, width=250)
text_chunks = re.split(r'(?<=[.!?])\s+', story_text.strip())
audio_segments = []

voice_type = "child"  # Options: "female", "male", "child"

def get_tts(text, voice_type):
    if voice_type == "male":
        return gTTS(text=text, lang="en", tld="co.uk", slow=False)  # British male voice
    elif voice_type == "child":
        return gTTS(text=text, lang="en", tld="com.au", slow=False)  # Australian child-like voice
    else:
        return gTTS(text=text, lang="en", tld="com", slow=False)  # Default female voice

for i, chunk in enumerate(text_chunks):
    print(f" * Generating speech for chunk {i+1}/{len(text_chunks)}...")

    # Generate speech using gTTS with selected voice type
    tts = get_tts(chunk, voice_type)
    filename = f"chunk_{i}.mp3"
    tts.save(filename)

    # Convert to WAV format for merging
    audio_segment = AudioSegment.from_mp3(filename)
    audio_segments.append(audio_segment)

# Step 3: Merge all Narration Chunks into One Audio File
final_narration = sum(audio_segments)
final_narration.export("story_narration.wav", format="wav")
files.download("story_narration.wav")

print("Speech synthesis completed!")

# Step 4: Generate Background Music with AudioLDM2
repo_id = "cvssp/audioldm2-music"  # High-quality model
pipe = AudioLDM2Pipeline.from_pretrained(repo_id, torch_dtype=torch.float16).to("cuda")

music_prompt = "An energetic jazz song with piano and saxophone"

print("Generating background music...")
music = pipe(
    prompt=music_prompt,
    num_inference_steps=200,
    audio_length_in_s=len(final_narration) / 1000,  # Match narration duration
    num_waveforms_per_prompt=1
).audios

# Save background music
scipy.io.wavfile.write("background_music.wav", rate=16000, data=music[0])
files.download("background_music.wav")
print(" Music generation completed!")

# ðŸŽ› Step 5: Apply Noise Reduction & Merging
final_narration = normalize(final_narration)
final_music = AudioSegment.from_wav("background_music.wav")

# Apply low-pass filter to remove high-frequency noise
final_narration = low_pass_filter(final_narration, 3000)

# Overlay narration on background music (reduce music volume by 10dB)
merged_audio = final_narration.overlay(final_music - 10)

# Step 6: Export the Final Audiobook
merged_audio.export("final_story_audio.wav", format="wav")
files.download("final_story_audio.wav")
print("Audiobook saved as 'final_story_audio.wav'")